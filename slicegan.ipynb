{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory Trained_Generators/NMC already exists. Enter new project name or hit enter to overwrite\n",
      "Loading Dataset...\n",
      "training image shape:  (252, 252, 253)\n",
      "dataset  0\n",
      "dataset  1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Volumes/tianjie 1/code/research/prj_3D_meta/2d-3d/SliceGAN/slicegan.ipynb Cell 1\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Volumes/tianjie%201/code/research/prj_3D_meta/2d-3d/SliceGAN/slicegan.ipynb#W0sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39m#Train\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Volumes/tianjie%201/code/research/prj_3D_meta/2d-3d/SliceGAN/slicegan.ipynb#W0sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39mif\u001b[39;00m Training:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Volumes/tianjie%201/code/research/prj_3D_meta/2d-3d/SliceGAN/slicegan.ipynb#W0sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     model\u001b[39m.\u001b[39;49mtrain(Project_path, image_type, data_type, data_path, netD, netG, img_channels, img_size, z_channels, scale_factor)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Volumes/tianjie%201/code/research/prj_3D_meta/2d-3d/SliceGAN/slicegan.ipynb#W0sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Volumes/tianjie%201/code/research/prj_3D_meta/2d-3d/SliceGAN/slicegan.ipynb#W0sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     img, raw, netG \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39mtest_img(Project_path, image_type, netG(), z_channels, lf\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m, periodic\u001b[39m=\u001b[39m[\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m])\n",
      "File \u001b[0;32m/Volumes/tianjie 1/code/research/prj_3D_meta/2d-3d/SliceGAN/slicegan/model.py:31\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(pth, imtype, datatype, real_data, Disc, Gen, nc, l, nz, sf)\u001b[0m\n\u001b[1;32m     28\u001b[0m     isotropic \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mLoading Dataset...\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m dataset_xyz \u001b[39m=\u001b[39m preprocessing\u001b[39m.\u001b[39;49mbatch(real_data, datatype, l, sf)\n\u001b[1;32m     33\u001b[0m \u001b[39m## Constants for NNs\u001b[39;00m\n\u001b[1;32m     34\u001b[0m matplotlib\u001b[39m.\u001b[39muse(\u001b[39m'\u001b[39m\u001b[39mAgg\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/Volumes/tianjie 1/code/research/prj_3D_meta/2d-3d/SliceGAN/slicegan/preprocessing.py:70\u001b[0m, in \u001b[0;36mbatch\u001b[0;34m(data, type, l, sf)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m             img1[img[x:x \u001b[39m+\u001b[39m l, y:y \u001b[39m+\u001b[39m l,lay] \u001b[39m==\u001b[39m phs] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> 70\u001b[0m         data[i, cnt, :, :] \u001b[39m=\u001b[39m img1[:,:]\n\u001b[1;32m     71\u001b[0m         \u001b[39m# data[i, (cnt+1)%3, :, :] = img1[:,:]\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[39mif\u001b[39;00m Testing:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### Welcome to SliceGAN ###\n",
    "####### Steve Kench #######\n",
    "'''\n",
    "Use this file to define your settings for a training run, or\n",
    "to generate a synthetic image using a trained generator.\n",
    "'''\n",
    "\n",
    "from slicegan import model, networks, util\n",
    "import argparse\n",
    "# Define project name\n",
    "Project_name = 'NMC'\n",
    "# Specify project folder.\n",
    "Project_dir = 'Trained_Generators'\n",
    "# Run with False to show an image during or after training\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('training', type=int)\n",
    "# args = parser.parse_args()\n",
    "# Training = args.training\n",
    "Training = 100\n",
    "\n",
    "Project_path = util.mkdr(Project_name, Project_dir, Training)\n",
    "\n",
    "## Data Processing\n",
    "# Define image  type (colour, grayscale, three-phase or two-phase.\n",
    "# n-phase materials must be segmented)\n",
    "image_type = 'nphase'\n",
    "# img_channels should be number of phases for nphase, 3 for colour, or 1 for grayscale\n",
    "img_channels = 3\n",
    "# define data type (for colour/grayscale images, must be 'colour' / '\n",
    "# greyscale. nphase can be, 'tif2D', 'png', 'jpg', tif3D, 'array')\n",
    "data_type = 'tif3D'\n",
    "# Path to your data. One string for isotrpic, 3 for anisotropic\n",
    "data_path = ['Examples/NMC.tif']\n",
    "\n",
    "## Network Architectures\n",
    "# Training image size, no. channels and scale factor vs raw data\n",
    "img_size, scale_factor = 64,  1\n",
    "# z vector depth\n",
    "z_channels = 32\n",
    "# Layers in G and D\n",
    "lays = 5\n",
    "laysd = 6\n",
    "dk, gk = [4]*laysd, [4]*lays                                    # kernal sizes\n",
    "# gk[0]=8\n",
    "ds, gs = [2]*laysd, [2]*lays                                    # strides\n",
    "# gs[0] = 4\n",
    "df, gf = [img_channels, 64, 128, 256, 512, 1], [\n",
    "    z_channels, 1024, 512, 128, 32, img_channels]  # filter sizes for hidden layers\n",
    "\n",
    "dp, gp = [1, 1, 1, 1, 0], [2, 2, 2, 2, 3]\n",
    "\n",
    "## Create Networks\n",
    "netD, netG = networks.slicegan_rc_nets(Project_path, Training, image_type, dk, ds, df,dp, gk ,gs, gf, gp)\n",
    "\n",
    "#Train\n",
    "if Training:\n",
    "    model.train(Project_path, image_type, data_type, data_path, netD, netG, img_channels, img_size, z_channels, scale_factor)\n",
    "else:\n",
    "    img, raw, netG = util.test_img(Project_path, image_type, netG(), z_channels, lf=8, periodic=[0, 1, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to run torchgraph see error message",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/Volumes/application/pyvenv/py311/lib/python3.11/site-packages/torchview/torchview.py:256\u001b[0m, in \u001b[0;36mforward_prop\u001b[0;34m(model, x, device, model_graph, mode, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(x, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[0;32m--> 256\u001b[0m     _ \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mto(device)(\u001b[39m*\u001b[39;49mx, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    257\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(x, Mapping):\n",
      "File \u001b[0;32m/Volumes/application/pyvenv/py311/lib/python3.11/site-packages/torchview/recorder_tensor.py:146\u001b[0m, in \u001b[0;36mmodule_forward_wrapper.<locals>._module_forward_wrapper\u001b[0;34m(mod, *args, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[39m# TODO: check if output contains RecorderTensor\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[39m# this seems not to be necessary so far\u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m out \u001b[39m=\u001b[39m _orig_module_forward(mod, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    148\u001b[0m model_graph\u001b[39m.\u001b[39mcontext_tracker[\u001b[39m'\u001b[39m\u001b[39mcurrent_depth\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m cur_depth\n",
      "File \u001b[0;32m/Volumes/application/pyvenv/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "File \u001b[0;32m/Volumes/tianjie 1/code/research/prj_3D_meta/2d-3d/SliceGAN/slicegan/networks.py:95\u001b[0m, in \u001b[0;36mslicegan_rc_nets.<locals>.Generator.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39mfor\u001b[39;00m lay, (conv, bn) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconvs[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m],\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbns[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])):\n\u001b[0;32m---> 95\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu_(bn(conv(x)))\n\u001b[1;32m     96\u001b[0m size \u001b[39m=\u001b[39m (\u001b[39mint\u001b[39m(x\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m]\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,)\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m,\u001b[39mint\u001b[39m(x\u001b[39m.\u001b[39mshape[\u001b[39m3\u001b[39m]\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,)\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m,\u001b[39mint\u001b[39m(x\u001b[39m.\u001b[39mshape[\u001b[39m3\u001b[39m]\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,)\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m/Volumes/application/pyvenv/py311/lib/python3.11/site-packages/torchview/recorder_tensor.py:146\u001b[0m, in \u001b[0;36mmodule_forward_wrapper.<locals>._module_forward_wrapper\u001b[0;34m(mod, *args, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[39m# TODO: check if output contains RecorderTensor\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[39m# this seems not to be necessary so far\u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m out \u001b[39m=\u001b[39m _orig_module_forward(mod, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    148\u001b[0m model_graph\u001b[39m.\u001b[39mcontext_tracker[\u001b[39m'\u001b[39m\u001b[39mcurrent_depth\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m cur_depth\n",
      "File \u001b[0;32m/Volumes/application/pyvenv/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "File \u001b[0;32m/Volumes/application/pyvenv/py311/lib/python3.11/site-packages/torch/nn/modules/conv.py:1108\u001b[0m, in \u001b[0;36mConvTranspose3d.forward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m   1104\u001b[0m output_padding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_padding(\n\u001b[1;32m   1105\u001b[0m     \u001b[39minput\u001b[39m, output_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel_size,  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   1106\u001b[0m     num_spatial_dims, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m-> 1108\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv_transpose3d(\n\u001b[1;32m   1109\u001b[0m     \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding,\n\u001b[1;32m   1110\u001b[0m     output_padding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation)\n",
      "File \u001b[0;32m/Volumes/application/pyvenv/py311/lib/python3.11/site-packages/torchview/recorder_tensor.py:241\u001b[0m, in \u001b[0;36mRecorderTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    239\u001b[0m     \u001b[39m# use original torch_function; otherwise,\u001b[39;00m\n\u001b[1;32m    240\u001b[0m     \u001b[39m# it leads to infinite recursive call of torch_function\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m__torch_function__(func, types, args, kwargs)\n\u001b[1;32m    243\u001b[0m \u001b[39m# if no RecorderTensor is found in input or output\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[39m# dont create any node, give the result only\u001b[39;00m\n",
      "File \u001b[0;32m/Volumes/application/pyvenv/py311/lib/python3.11/site-packages/torch/_tensor.py:1295\u001b[0m, in \u001b[0;36mTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1294\u001b[0m \u001b[39mwith\u001b[39;00m _C\u001b[39m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[0;32m-> 1295\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1296\u001b[0m     \u001b[39mif\u001b[39;00m func \u001b[39min\u001b[39;00m get_default_nowrap_functions():\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given transposed=1, weight of size [32, 1024, 4, 4, 4], expected input[1, 1024, 4, 4, 4] to have 32 channels, but got 1024 channels instead",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Volumes/tianjie 1/code/research/prj_3D_meta/2d-3d/SliceGAN/slicegan.ipynb Cell 2\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Volumes/tianjie%201/code/research/prj_3D_meta/2d-3d/SliceGAN/slicegan.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Volumes/tianjie%201/code/research/prj_3D_meta/2d-3d/SliceGAN/slicegan.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchview\u001b[39;00m \u001b[39mimport\u001b[39;00m draw_graph\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Volumes/tianjie%201/code/research/prj_3D_meta/2d-3d/SliceGAN/slicegan.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model_graph \u001b[39m=\u001b[39m draw_graph(netG(), input_size\u001b[39m=\u001b[39;49m(\u001b[39m1024\u001b[39;49m, \u001b[39m4\u001b[39;49m, \u001b[39m4\u001b[39;49m, \u001b[39m4\u001b[39;49m), expand_nested\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Volumes/tianjie%201/code/research/prj_3D_meta/2d-3d/SliceGAN/slicegan.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model_graph\u001b[39m.\u001b[39mvisual_graph\n",
      "File \u001b[0;32m/Volumes/application/pyvenv/py311/lib/python3.11/site-packages/torchview/torchview.py:220\u001b[0m, in \u001b[0;36mdraw_graph\u001b[0;34m(model, input_data, input_size, graph_name, depth, device, dtypes, mode, strict, expand_nested, graph_dir, hide_module_functions, hide_inner_tensors, roll, show_shapes, save_graph, filename, directory, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m input_recorder_tensor, kwargs_record_tensor, input_nodes \u001b[39m=\u001b[39m process_input(\n\u001b[1;32m    212\u001b[0m     input_data, input_size, kwargs, device, dtypes\n\u001b[1;32m    213\u001b[0m )\n\u001b[1;32m    215\u001b[0m model_graph \u001b[39m=\u001b[39m ComputationGraph(\n\u001b[1;32m    216\u001b[0m     visual_graph, input_nodes, show_shapes, expand_nested,\n\u001b[1;32m    217\u001b[0m     hide_inner_tensors, hide_module_functions, roll, depth\n\u001b[1;32m    218\u001b[0m )\n\u001b[0;32m--> 220\u001b[0m forward_prop(\n\u001b[1;32m    221\u001b[0m     model, input_recorder_tensor, device, model_graph,\n\u001b[1;32m    222\u001b[0m     model_mode, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs_record_tensor\n\u001b[1;32m    223\u001b[0m )\n\u001b[1;32m    225\u001b[0m model_graph\u001b[39m.\u001b[39mfill_visual_graph()\n\u001b[1;32m    227\u001b[0m \u001b[39mif\u001b[39;00m save_graph:\n",
      "File \u001b[0;32m/Volumes/application/pyvenv/py311/lib/python3.11/site-packages/torchview/torchview.py:264\u001b[0m, in \u001b[0;36mforward_prop\u001b[0;34m(model, x, device, model_graph, mode, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnknown input type\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    263\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 264\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    265\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFailed to run torchgraph see error message\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    266\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    268\u001b[0m     model\u001b[39m.\u001b[39mtrain(saved_model_mode)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to run torchgraph see error message"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchview import draw_graph\n",
    "\n",
    "model_graph = draw_graph(netG(), input_size=(1024, 4, 4, 4), expand_nested=True)\n",
    "model_graph.visual_graph\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
